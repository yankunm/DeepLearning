{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ce0d7f",
   "metadata": {},
   "source": [
    "# Problem 2: Multinomial Logistic Regression from Pre-trained Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a93d6e",
   "metadata": {},
   "source": [
    "In this problem, you will implement a logistic regression model from scratch to classify MNIST given a pre-trained feature extractor. Specifically, you will walk through the following steps:\n",
    "\n",
    "**Step 1:** Load the pre-trained weights to your feature extractor. The code defining the architecture of your feature extractor is included in the **attachment**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e327ab",
   "metadata": {},
   "source": [
    "**Step 2:** Extract the latent representation (denoted as $h \\in \\mathbb{R}^{k}$) from each sample in MNIST (denoted as $x \\in \\mathbb{R}^{784}$) using the pre-trained feature extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3bdb5",
   "metadata": {},
   "source": [
    "**Step 3:** Derive the gradient of W and b with respect to the *cross-entropy* loss between the label and prediction $\\hat{y}$ of the model:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\sigma ( W^{T} h + b),\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\sigma(\\cdot)$ denotes the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62adf6d",
   "metadata": {},
   "source": [
    "**Step 4**: Manually implement Stochastic Gradient Descent (SGD) from *scratch (using matrix operations only)*. Use the gradient derived in **Step 3** to run training and update the parameters W and b. Finally, report the classification accuracy on the test set."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
