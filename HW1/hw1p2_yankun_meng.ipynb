{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ce0d7f",
   "metadata": {},
   "source": [
    "# Problem 2: Multinomial Logistic Regression from Pre-trained Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b71dfda",
   "metadata": {},
   "source": [
    "Author: Yankun (Alex) Meng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from pretrained_model.Encoder import extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c304b4f4",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a93d6e",
   "metadata": {},
   "source": [
    "In this problem, you will implement a logistic regression model from scratch to classify MNIST given a pre-trained feature extractor. Specifically, you will walk through the following steps:\n",
    "\n",
    "**Step 1:** Load the pre-trained weights to your feature extractor. The code defining the architecture of your feature extractor is included in the **attachment**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc833f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "device = \"cpu\" # i will just use cpu\n",
    "feature_extractor = extractor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79de3c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pretrained weights into the extractor\n",
    "state_dict = torch.load(\"./pretrained_model/feature_extractor_weights.pth\", map_location=device)\n",
    "feature_extractor.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404f9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.eval()\n",
    "\n",
    "for p in feature_extractor.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfa712",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e327ab",
   "metadata": {},
   "source": [
    "**Step 2:** Extract the latent representation (denoted as $h \\in \\mathbb{R}^{k}$) from each sample in MNIST (denoted as $x \\in \\mathbb{R}^{784}$) using the pre-trained feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0fe8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba58ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST from torchvision\n",
    "train = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3db8f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "x, y = train[0] # (image, label)\n",
    "print(x.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0adbdc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Loader wrappers\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test, batch_size=64, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e20d5",
   "metadata": {},
   "source": [
    "Now we are ready to extract the features using our pretrained feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02cf36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (H, Y)\n",
    "H_train, Y_train = [], []\n",
    "H_test,  Y_test  = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0bd201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop Assignment\n",
    "with torch.no_grad():\n",
    "    # train set\n",
    "    for x, y in trainloader:\n",
    "        h = feature_extractor(x)\n",
    "        H_train.append(h)\n",
    "        Y_train.append(y)\n",
    "\n",
    "    # test set\n",
    "    for x, y in testloader:\n",
    "        h = feature_extractor(x)\n",
    "        H_test.append(h)\n",
    "        Y_test.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80cd4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 256])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 256])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all batches\n",
    "H_train = torch.cat(H_train, dim=0)\n",
    "Y_train = torch.cat(Y_train, dim=0)\n",
    "H_test = torch.cat(H_test, dim=0)\n",
    "Y_test = torch.cat(Y_test, dim=0)\n",
    "print(H_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(H_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3bdb5",
   "metadata": {},
   "source": [
    "**Step 3:** Derive the gradient of W and b with respect to the *cross-entropy* loss between the label and prediction $\\hat{y}$ of the model:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\sigma ( W^{T} h + b),\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\sigma(\\cdot)$ denotes the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd22d7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mathbf{y}} = \\sigma(\\mathbf{z}) = \\sigma(\\mathbf{W}^\\top \\mathbf{h} + \\mathbf{b})\n",
    "$$\n",
    "\n",
    "For a single sample with one-hot label $\\mathbf{y} \\in \\mathbb{R}^C$, the cross-entropy loss is\n",
    "\n",
    "$$\n",
    "L = - \\sum_{c=1}^{C} y_c \\log \\hat{y}_c\n",
    "$$\n",
    "\n",
    "For a batch of $N$ samples, the average loss is\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log \\hat{y}_{i,c}\n",
    "$$\n",
    "\n",
    "\n",
    "We first compute the derivative of the loss with respect to the input to softmax $\\mathbf{z}$\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y}_c &= \\frac{e^{z_c}}{\\sum_{j=1}^{C} e^{z_j}} \\\\\n",
    "L &= - \\sum_{k=1}^{C} y_k \\log \\hat{y}_k \\\\\n",
    "\\frac{\\partial L}{\\partial z_c} &= \\sum_{k=1}^{C} \\frac{\\partial L}{\\partial \\hat{y}_k} \\frac{\\partial \\hat{y}_k}{\\partial z_c} \\\\\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_k} &= - \\frac{y_k}{\\hat{y}_k} \\\\\n",
    "\\frac{\\partial \\hat{y}_k}{\\partial z_c} &= \n",
    "\\begin{cases} \n",
    "\\hat{y}_c (1 - \\hat{y}_c), & k=c \\\\\n",
    "- \\hat{y}_k \\hat{y}_c, & k \\neq c\n",
    "\\end{cases} \\\\\n",
    "\\frac{\\partial L}{\\partial z_c} &= - \\frac{y_c}{\\hat{y}_c} \\cdot \\hat{y}_c (1 - \\hat{y}_c) + \\sum_{k \\neq c} -\\frac{y_k}{\\hat{y}_k}(-\\hat{y}_k \\hat{y}_c) \\\\\n",
    "&= \\hat{y}_c - y_c\n",
    "\\end{align*}\n",
    "\n",
    "Vectorized for the batch\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} = \\hat{\\mathbf{Y}} - \\mathbf{Y},\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mathbf{Y}}, \\mathbf{Y} \\in \\mathbb{R}^{N \\times C}$.\n",
    "\n",
    "Using the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{Z}} \\cdot \\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}}\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{Z} = \\mathbf{H} \\mathbf{W} + \\mathbf{1}_N \\mathbf{b}^\\top$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} = \\mathbf{H}^\\top\n",
    "$$\n",
    "\n",
    "Thus, the gradient w.r.t. weights for the batch is\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{1}{N} \\mathbf{H}^\\top (\\hat{\\mathbf{Y}} - \\mathbf{Y})}\n",
    "$$\n",
    "\n",
    "Similarly, the gradient w.r.t. bias is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{Z}} \\cdot \\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}}\n",
    "$$\n",
    "\n",
    "Since $\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}} = \\mathbf{1}_N$ (broadcasted sum over batch)\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{\\mathbf{y}}_i - \\mathbf{y}_i)}\n",
    "$$\n",
    "\n",
    "with resulting shape $\\mathbb{R}^{C}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62adf6d",
   "metadata": {},
   "source": [
    "**Step 4**: Manually implement Stochastic Gradient Descent (SGD) from *scratch (using matrix operations only)*. Use the gradient derived in **Step 3** to run training and update the parameters W and b. Finally, report the classification accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "800b2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_classes = 10\n",
    "num_features = H_train.shape[1]  # 256 from extractor\n",
    "lr = 0.1\n",
    "num_epochs = 20\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "014099b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and bias\n",
    "W = torch.zeros((num_features, num_classes), dtype=torch.float32)\n",
    "b = torch.zeros(num_classes, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a11831cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot\n",
    "def to_one_hot(y, C):\n",
    "    return torch.eye(C)[y]  # shape: [N, C]\n",
    "\n",
    "Y_train_onehot = to_one_hot(Y_train, num_classes)\n",
    "Y_test_onehot  = to_one_hot(Y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f77ef68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(z):\n",
    "    # subtract max for numerical stability\n",
    "    exp_z = torch.exp(z - z.max(dim=1, keepdim=True)[0])\n",
    "    return exp_z / exp_z.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0449a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "def cross_entropy(pred, target):\n",
    "    return - (target * torch.log(pred + 1e-8)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14fa5e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0281\n",
      "Epoch 2/20, Loss: 0.0181\n",
      "Epoch 3/20, Loss: 0.0139\n",
      "Epoch 4/20, Loss: 0.0115\n",
      "Epoch 5/20, Loss: 0.0099\n",
      "Epoch 6/20, Loss: 0.0087\n",
      "Epoch 7/20, Loss: 0.0079\n",
      "Epoch 8/20, Loss: 0.0072\n",
      "Epoch 9/20, Loss: 0.0066\n",
      "Epoch 10/20, Loss: 0.0061\n",
      "Epoch 11/20, Loss: 0.0057\n",
      "Epoch 12/20, Loss: 0.0054\n",
      "Epoch 13/20, Loss: 0.0051\n",
      "Epoch 14/20, Loss: 0.0048\n",
      "Epoch 15/20, Loss: 0.0046\n",
      "Epoch 16/20, Loss: 0.0044\n",
      "Epoch 17/20, Loss: 0.0042\n",
      "Epoch 18/20, Loss: 0.0040\n",
      "Epoch 19/20, Loss: 0.0038\n",
      "Epoch 20/20, Loss: 0.0037\n"
     ]
    }
   ],
   "source": [
    "# Number of training samples\n",
    "num_train = H_train.shape[0]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle dataset\n",
    "    perm = torch.randperm(num_train)\n",
    "    H_shuffled = H_train[perm]\n",
    "    Y_shuffled = Y_train_onehot[perm]\n",
    "\n",
    "    # Mini-batch SGD\n",
    "    for i in range(0, num_train, batch_size):\n",
    "        H_batch = H_shuffled[i:i+batch_size]  # [batch, features]\n",
    "        Y_batch = Y_shuffled[i:i+batch_size]  # [batch, C]\n",
    "\n",
    "        # Forward pass: linear + softmax\n",
    "        z = H_batch @ W + b           # [batch, C]\n",
    "        y_hat = softmax(z)            # [batch, C]\n",
    "\n",
    "        # Compute gradients manually (from Step 3)\n",
    "        dW = H_batch.T @ (y_hat - Y_batch) / H_batch.shape[0]  # [features, C]\n",
    "        db = (y_hat - Y_batch).mean(dim=0)                     # [C]\n",
    "\n",
    "        # Update parameters\n",
    "        W -= lr * dW\n",
    "        b -= lr * db\n",
    "\n",
    "    # Compute training loss per epoch\n",
    "    z_train = H_train @ W + b\n",
    "    y_hat_train = softmax(z_train)\n",
    "    loss = cross_entropy(y_hat_train, Y_train_onehot)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5014e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set classification accuracy: 99.08%\n"
     ]
    }
   ],
   "source": [
    "# ---- Evaluate on test set ----\n",
    "z_test = H_test @ W + b\n",
    "y_hat_test = softmax(z_test)\n",
    "y_pred = torch.argmax(y_hat_test, dim=1)\n",
    "\n",
    "accuracy = (y_pred == Y_test).float().mean()\n",
    "print(f\"Test set classification accuracy: {accuracy.item() * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tidy3d_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
